{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.helpers import display_verses_with_codepoints\n",
    "display_verses_with_codepoints([\"0x06D7\", \"0x06DA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "إِنَّ شَجَرَتَ ٱلزَّقُّومِ ‏﴿٤٣﴾‏ طَعَامُ ٱلۡأَثِيمِ ‏﴿٤٤﴾‏\n",
      "ʔiña ʃaʒarata zzaqqu:m tˤaʕa:mu lʔaθi:m\n",
      "ʔiña ʃaʒarata zzaqqu:mi tˤaʕa:mu lʔaθi:m\n"
     ]
    }
   ],
   "source": [
    "from core.phonemizer import Phonemizer\n",
    "\n",
    "pm = Phonemizer()\n",
    "ref = \"44:43 - 44:44\"\n",
    "\n",
    "res = pm.phonemize(ref, stops=[\"verse\"])\n",
    "print(res.text)\n",
    "print(res.phonemes_str(phoneme_sep=\"\", word_sep=\" \", verse_sep=\"\"))\n",
    "\n",
    "res = pm.phonemize(ref, stops=[])\n",
    "print(res.phonemes_str(phoneme_sep=\"\", word_sep=\" \", verse_sep=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116 cross-word Idghām Mutamāthilayn hits\n",
      "Wrote 116 pairs to data/mutamathilayn.txt\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIG -----------------------------------------------------------\n",
    "QURAN_JSON = \"data/Quran.json\"        # ↙ update if the file lives elsewhere\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "import json, re, unicodedata\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "TAG_RE = re.compile(r\"</?rule[^>]*?>\", flags=re.IGNORECASE)\n",
    "HARAKAT_RNG = '[\\u064B-\\u0650]'          # fatḥatan–kasra  (U+064B..U+0650)\n",
    "SHADDA = '\\u0651'\n",
    "LAM    = '\\u0644'\n",
    "MEEM   = '\\u0645'\n",
    "NUN    = '\\u0646'\n",
    "\n",
    "import unicodedata, re\n",
    "\n",
    "COMBINING = re.compile(r'[\\u064B-\\u0652\\u0651]')      # tanwīn + harakāt + sukūn + shadda\n",
    "\n",
    "def last_bare_consonant(word: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Return the last consonant **only if** it is not followed by\n",
    "    ANY Arabic combining mark (tanwīn, short vowel, sukūn, shadda).\n",
    "    Otherwise return None.\n",
    "    \"\"\"\n",
    "    # walk backwards through the string\n",
    "    for i in range(len(word) - 1, -1, -1):\n",
    "        ch = word[i]\n",
    "        if '\\u0621' <= ch <= '\\u064A':            # Arabic base letter\n",
    "            if i == len(word) - 1 or not COMBINING.match(word[i + 1]):\n",
    "                return ch                         # bare consonant found\n",
    "            return None                           # consonant has a mark → reject\n",
    "    return None                                   # no consonant at all\n",
    "\n",
    "\n",
    "def strip_tags(text: str) -> str:\n",
    "    \"\"\"Drop <rule …> wrappers; JSON sometimes stores text as list-segments.\"\"\"\n",
    "    if isinstance(text, list):\n",
    "        text = \"\".join(text)\n",
    "    return TAG_RE.sub(\"\", text)\n",
    "\n",
    "def base_letters(word: str) -> str:\n",
    "    \"\"\"Remove Arabic combining marks to leave only consonant code-points.\"\"\"\n",
    "    return \"\".join(ch for ch in word if not unicodedata.combining(ch))\n",
    "\n",
    "def last_base_letter(word: str) -> str | None:\n",
    "    \"\"\"Return last consonant (ignoring harakāt) or None.\"\"\"\n",
    "    bases = [ch for ch in base_letters(word) if \"\\u0621\" <= ch <= \"\\u064A\"]\n",
    "    return bases[-1] if bases else None\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "data = json.loads(Path(QURAN_JSON).read_text(encoding=\"utf-8\"))\n",
    "# order by running index so consecutive items are truly “next word”\n",
    "words = sorted(data.values(), key=lambda d: d[\"word_index\"])\n",
    "\n",
    "pairs: list[tuple[str, str, str, str]] = []\n",
    "\n",
    "for w1, w2 in zip(words, words[1:]):\n",
    "    txt1, txt2 = strip_tags(w1[\"text\"]), strip_tags(w2[\"text\"])\n",
    "    last = last_bare_consonant(txt1)\n",
    "    if not last or last in [MEEM, NUN]:                       \n",
    "        continue\n",
    "    # does the next word begin with  last + shadda + vowel ?\n",
    "    if re.match(fr\"{re.escape(last)}{SHADDA}{HARAKAT_RNG}\", txt2):\n",
    "        pairs.append((w1[\"location\"], txt1, w2[\"location\"], txt2))\n",
    "\n",
    "# --- Display ----------------------------------------------------------\n",
    "df = pd.DataFrame(pairs, columns=[\"Ref-A\", \"Word-A\", \"Ref-B\", \"Word-B\"])\n",
    "print(f\"Found {len(df)} cross-word Idghām Mutamāthilayn hits\")\n",
    "df\n",
    "# write the df to file\n",
    "\n",
    "# --- Dump to a plain-text file --------------------------------------\n",
    "outfile = \"data/mutamathilayn.txt\"\n",
    "\n",
    "with open(outfile, \"w\", encoding=\"utf-8\") as fh:\n",
    "    for _, r in df.iterrows():\n",
    "        fh.write(f\"{r['Ref-A']}\\t{r['Word-A']}\\t→\\t{r['Ref-B']}\\t{r['Word-B']}\\n\")\n",
    "\n",
    "print(f\"Wrote {len(df)} pairs to {outfile}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tagged 116 first-consonants → data/Quran_v22.json\n"
     ]
    }
   ],
   "source": [
    "# === Cell : tag *first* consonant of cross-word Idghām Mutamāthilayn Ṣaghīr ======\n",
    "\n",
    "\"\"\"\n",
    "• Reads  : data/Quran.json\n",
    "• Needs  : data/mutamathilayn.txt   (refA<TAB>wordA<TAB>→<TAB>refB<TAB>wordB)\n",
    "• Writes : data/Quran_idgham.json\n",
    "   – Adds <rule class=idgham_mutamathilayn>…</rule> only on the *silent* lam/meem-free\n",
    "     consonant at the end of word-A.\n",
    "\"\"\"\n",
    "\n",
    "import json, re, unicodedata\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "QURAN_IN   = \"data/Quran.json\"\n",
    "PAIRS_IN   = \"data/mutamathilayn.txt\"\n",
    "QURAN_OUT  = \"data/Quran_v22.json\"\n",
    "\n",
    "TAG_RE = re.compile(r\"</?rule[^>]*?>\", flags=re.I)\n",
    "\n",
    "# ------------------------------------------------------------------ helpers\n",
    "def last_bare_idx(text: str) -> int | None:\n",
    "    \"\"\"index of last consonant with **no** following mark; honours rule-tags.\"\"\"\n",
    "    i = len(text) - 1\n",
    "    while i >= 0:\n",
    "        if text[i] == \">\":                 # walk back over a tag\n",
    "            i = text.rfind(\"<\", 0, i) - 1\n",
    "            continue\n",
    "        ch = text[i]\n",
    "        if \"\\u0621\" <= ch <= \"\\u064A\":     # Arabic base letter\n",
    "            j = i + 1\n",
    "            while j < len(text) and unicodedata.combining(text[j]):\n",
    "                j += 1\n",
    "            if j >= len(text) or text[j] == \"<\":     # nothing after → bare\n",
    "                return i\n",
    "        i -= 1\n",
    "    return None\n",
    "\n",
    "def tag_word_a(rec: dict) -> None:\n",
    "    \"\"\"Wrap the last bare consonant of word-A only.\"\"\"\n",
    "    t = rec[\"text\"]\n",
    "    idx = last_bare_idx(t)\n",
    "    if idx is not None:\n",
    "        rec[\"text\"] = (\n",
    "            f\"{t[:idx]}<rule class=idgham_mutamathilayn>{t[idx]}</rule>{t[idx+1:]}\"\n",
    "        )\n",
    "\n",
    "# ------------------------------------------------------------------ main\n",
    "with open(QURAN_IN, encoding=\"utf-8\") as fh:\n",
    "    quran = json.load(fh)\n",
    "\n",
    "pairs: list[tuple[str, str]] = []\n",
    "with open(PAIRS_IN, encoding=\"utf-8\") as fh:\n",
    "    for ln in fh:\n",
    "        if ln.strip():\n",
    "            ref_a, _, _, ref_b, _ = ln.rstrip(\"\\n\").split(\"\\t\")\n",
    "            pairs.append((ref_a, ref_b))\n",
    "\n",
    "quran_out = deepcopy(quran)\n",
    "for ref_a, _ in pairs:          # only word-A modified\n",
    "    tag_word_a(quran_out[ref_a])\n",
    "\n",
    "with open(QURAN_OUT, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(quran_out, fh, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✓ Tagged {len(pairs)} first-consonants → {QURAN_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell : detect Lam Shamsiyyah (silent article-lam) ==========================\n",
    "\n",
    "\"\"\"\n",
    "Criteria:\n",
    "  • LAM (ل) with *no* sukūn / vowel mark\n",
    "  • immediately followed by a SUN-LETTER (ت ث د ذ ر ز س ش ص ض ط ظ ل ن)\n",
    "    that carries a SHADDA\n",
    "This cell:\n",
    "  1. loads data/Quran.json\n",
    "  2. finds every word that meets the pattern\n",
    "  3. writes tab-separated list to data/lam_shamsi.txt\n",
    "  4. shows first rows as a DataFrame\n",
    "\"\"\"\n",
    "\n",
    "import json, re, unicodedata, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ── paths ───────────────────────────────────────────────────────────────────\n",
    "IN_JSON = \"data/Quran.json\"\n",
    "OUT_TXT = \"data/lam_shams.txt\"\n",
    "\n",
    "# ── glyphs & helpers ────────────────────────────────────────────────────────\n",
    "LAM        = \"\\u0644\"\n",
    "SHADDA     = \"\\u0651\"\n",
    "SUN        = set(\"تثدذرزسشصضطظللن\")\n",
    "\n",
    "COMB_RE = re.compile(r\"[\\u064B-\\u0652\\u0651]\")       # all harakāt + sukūn + shadda\n",
    "TAG_RE  = re.compile(r\"</?rule[^>]*?>\", flags=re.I)\n",
    "\n",
    "def strip_tags(x: str) -> str:\n",
    "    return TAG_RE.sub(\"\", \"\".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "def has_lam_shamsi(word: str) -> bool:\n",
    "    w = strip_tags(word)\n",
    "    i = 0\n",
    "    while i < len(w):\n",
    "        if w[i] == LAM and (i + 1 == len(w) or not unicodedata.combining(w[i + 1])):\n",
    "            # next base letter\n",
    "            j = i + 1\n",
    "            while j < len(w) and unicodedata.combining(w[j]):\n",
    "                j += 1\n",
    "            if j < len(w) and w[j] in SUN:\n",
    "                return True\n",
    "        # skip over tags cleanly\n",
    "        i = w.find(\">\", i) + 1 if w[i] == \"<\" else i + 1\n",
    "    return False\n",
    "\n",
    "# ── scan Qur’an ─────────────────────────────────────────────────────────────\n",
    "data = json.loads(Path(IN_JSON).read_text(encoding=\"utf-8\"))\n",
    "hits = [(ref, strip_tags(d[\"text\"])) for ref, d in data.items() if has_lam_shamsi(d[\"text\"])]\n",
    "\n",
    "print(f\"Found {len(hits)} lam-shamsiyyah words\")\n",
    "\n",
    "# write file\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ref, word in hits:\n",
    "        f.write(f\"{ref}\\t{word}\\n\")\n",
    "print(f\"Saved list → {OUT_TXT}\")\n",
    "\n",
    "# preview\n",
    "pd.DataFrame(hits, columns=[\"Reference\", \"Word\"]).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell : wrap article-lam with <rule class=lam_shamsiyah> =====================\n",
    "\n",
    "\"\"\"\n",
    "• Reads  : data/Quran.json  +  data/lam_shamsi.txt (list from earlier cell)\n",
    "• Writes : data/Quran_lam_shamsi.json\n",
    "   – Inserts <rule class=lam_shamsiyah>…</rule> around the **lam**\n",
    "   – Skips words already tagged\n",
    "\"\"\"\n",
    "\n",
    "import json, re, unicodedata\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "# — paths —\n",
    "IN_JSON  = \"data/Quran_v2.json\"\n",
    "LIST_TXT = \"data/lam_shams.txt\"\n",
    "OUT_JSON = \"data/Quran_v2_lam_shams.json\"\n",
    "\n",
    "# — glyphs —\n",
    "LAM, SHADDA = \"\\u0644\", \"\\u0651\"\n",
    "SUN = set(\"تثدذرزسشصضطظللن\")\n",
    "\n",
    "TAG_RE = re.compile(r\"</?rule[^>]*?>\", flags=re.I)\n",
    "\n",
    "def strip_tags(t: str) -> str:\n",
    "    return TAG_RE.sub(\"\", \"\".join(t) if isinstance(t, list) else t)\n",
    "\n",
    "def add_tag(text: str) -> (str, bool):\n",
    "    \"\"\"Wrap first silent article-lam (if not already tagged).\"\"\"\n",
    "    if \"class=laam_shamsiyah\" in text:\n",
    "        return text, True\n",
    "\n",
    "    s = text\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        if s[i] == \"<\":\n",
    "            i = s.find(\">\", i) + 1\n",
    "            continue\n",
    "\n",
    "        if s[i] == LAM and (i + 1 == len(s) or not unicodedata.combining(s[i + 1])):\n",
    "            # find next base letter\n",
    "            j = i + 1\n",
    "            while j < len(s) and unicodedata.combining(s[j]):\n",
    "                j += 1\n",
    "            if j < len(s) and s[j] in SUN:\n",
    "                # # confirm shadda on that sun-letter\n",
    "                # k = j + 1\n",
    "                # while k < len(s) and unicodedata.combining(s[k]):\n",
    "                #     if s[k] == SHADDA:\n",
    "                return f\"{s[:i]}<rule class=laam_shamsiyah>{LAM}</rule>{s[i+1:]}\", True\n",
    "                    # k += 1\n",
    "        i += 1\n",
    "    return text, False\n",
    "\n",
    "# — load data & list —\n",
    "quran = json.loads(Path(IN_JSON).read_text(encoding=\"utf-8\"))\n",
    "refs  = {ln.split(\"\\t\")[0] for ln in Path(LIST_TXT).read_text(\"utf-8\").splitlines() if ln.strip()}\n",
    "\n",
    "quran_new = deepcopy(quran)\n",
    "num_tagged = 0\n",
    "skipped = 0\n",
    "for ref in refs:\n",
    "    text, tagged = add_tag(quran_new[ref][\"text\"])\n",
    "    if tagged:\n",
    "        quran_new[ref][\"text\"] = text\n",
    "        num_tagged += 1\n",
    "    else:\n",
    "        skipped += 1\n",
    "        print(f\"skipped {ref}\")\n",
    "\n",
    "# — write —\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(quran_new, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✓ Skipped {skipped} words\")\n",
    "print(f\"✓ Tagged {num_tagged} words\")\n",
    "print(f\"✓ Tagged {len(refs)} lam-shamsiyyah words → {OUT_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell : final Lam-Shamsiyyah tagging (handles inner tags & tatwīl) ============\n",
    "\n",
    "import json, re, unicodedata\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "IN_JSON   = \"data/Quran_v2.json\"\n",
    "LIST_TXT  = \"data/lam_shams.txt\"          # 5 283 refs\n",
    "OUT_JSON  = \"data/Quran_v2_lam_shams.json\"\n",
    "\n",
    "LAM       = \"\\u0644\"\n",
    "SHADDA    = \"\\u0651\"\n",
    "TATWEEL   = \"\\u0640\"\n",
    "SUN       = set(\"تثدذرزسشصضطظللن\")\n",
    "TAG_RE    = re.compile(r\"</?rule[^>]*?>\", flags=re.I)\n",
    "\n",
    "def strip_tags(x: str) -> str:\n",
    "    return TAG_RE.sub(\"\", \"\".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "def tag_lam(word: str) -> (str, bool):\n",
    "    if \"class=laam_shamsiyah\" in word:\n",
    "        return word, True\n",
    "\n",
    "    s = word\n",
    "    cand = None\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        ch = s[i]\n",
    "        if ch == \"<\":                          # skip tag\n",
    "            i = s.find(\">\", i) + 1\n",
    "            continue\n",
    "\n",
    "        if ch == LAM and (i + 1 == len(s) or not unicodedata.combining(s[i + 1])):\n",
    "            j = i + 1\n",
    "            while j < len(s):\n",
    "                cj = s[j]\n",
    "                if cj == \"<\":\n",
    "                    j = s.find(\">\", j) + 1\n",
    "                    continue\n",
    "                if cj == TATWEEL or unicodedata.combining(cj):\n",
    "                    j += 1\n",
    "                    continue\n",
    "                break\n",
    "            if j < len(s) and s[j] in SUN:\n",
    "                cand = i          # keep the _last_ matching lam\n",
    "        i += 1\n",
    "\n",
    "    if cand is not None:\n",
    "        idx = cand\n",
    "        return (\n",
    "            f\"{s[:idx]}<rule class=laam_shamsiyah>{LAM}</rule>{s[idx+1:]}\",\n",
    "            True,\n",
    "        )\n",
    "    return word, False\n",
    "\n",
    "# ── run tagging ─────────────────────────────────────────────────────────\n",
    "quran = json.loads(Path(IN_JSON).read_text(encoding=\"utf-8\"))\n",
    "refs  = {ln.split(\"\\t\")[0] for ln in Path(LIST_TXT).read_text().splitlines() if ln.strip()}\n",
    "\n",
    "quran_new, tagged, skipped = deepcopy(quran), 0, 0\n",
    "for ref in refs:\n",
    "    txt, ok = tag_lam(quran_new[ref][\"text\"])\n",
    "    quran_new[ref][\"text\"] = txt\n",
    "    tagged, skipped = tagged + ok, skipped + (not ok)\n",
    "\n",
    "# ── save & report ───────────────────────────────────────────────────────\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(quran_new, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Tagged : {tagged}\")\n",
    "print(f\"Skipped: {skipped}  (should be 0)\")\n",
    "print(f\"→ {OUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>word</th>\n",
       "      <th>phonemes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7:145:1</td>\n",
       "      <td>وَكَتَبۡنَا</td>\n",
       "      <td>wakatabQna:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7:145:2</td>\n",
       "      <td>لَهُۥ</td>\n",
       "      <td>lahu:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7:145:3</td>\n",
       "      <td>فِى</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7:145:4</td>\n",
       "      <td>ٱلۡأَلۡوَاحِ</td>\n",
       "      <td>lʔalwa:ħi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7:145:5</td>\n",
       "      <td>مِن</td>\n",
       "      <td>miŋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7:145:6</td>\n",
       "      <td>كُلِّ</td>\n",
       "      <td>kulli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7:145:7</td>\n",
       "      <td>شَىۡءٍ</td>\n",
       "      <td>ʃajʔi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7:145:8</td>\n",
       "      <td>مَّوۡعِظَةً</td>\n",
       "      <td>m̃awʕiðˤata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7:145:9</td>\n",
       "      <td>وَتَفۡصِيلاً</td>\n",
       "      <td>w̃atafsˤi:la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7:145:10</td>\n",
       "      <td>لِّكُلِّ</td>\n",
       "      <td>llikulli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7:145:11</td>\n",
       "      <td>شَىۡءٍ</td>\n",
       "      <td>ʃajʔiŋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7:145:12</td>\n",
       "      <td>فَخُذۡهَا</td>\n",
       "      <td>faxuðha:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7:145:13</td>\n",
       "      <td>بِقُوَّةٍ</td>\n",
       "      <td>biquwwati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7:145:14</td>\n",
       "      <td>وَأۡمُرۡ</td>\n",
       "      <td>w̃aʔmur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7:145:15</td>\n",
       "      <td>قَوۡمَكَ</td>\n",
       "      <td>qawmaka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7:145:16</td>\n",
       "      <td>يَأۡخُذُواۡ</td>\n",
       "      <td>jaʔxuðu:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7:145:17</td>\n",
       "      <td>بِأَحۡسَنِهَا‌ۚ</td>\n",
       "      <td>biʔaħsaniha:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7:145:18</td>\n",
       "      <td>سَأُوۡرِيكُمۡ</td>\n",
       "      <td>saʔuri:kum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7:145:19</td>\n",
       "      <td>دَارَ</td>\n",
       "      <td>da:ra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7:145:20</td>\n",
       "      <td>ٱلۡفَـٰسِقِينَ</td>\n",
       "      <td>lfa:siqi:n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    location             word      phonemes\n",
       "0    7:145:1      وَكَتَبۡنَا   wakatabQna:\n",
       "1    7:145:2            لَهُۥ         lahu:\n",
       "2    7:145:3              فِى            fi\n",
       "3    7:145:4     ٱلۡأَلۡوَاحِ     lʔalwa:ħi\n",
       "4    7:145:5              مِن           miŋ\n",
       "5    7:145:6            كُلِّ         kulli\n",
       "6    7:145:7           شَىۡءٍ         ʃajʔi\n",
       "7    7:145:8      مَّوۡعِظَةً   m̃awʕiðˤata\n",
       "8    7:145:9     وَتَفۡصِيلاً  w̃atafsˤi:la\n",
       "9   7:145:10         لِّكُلِّ      llikulli\n",
       "10  7:145:11           شَىۡءٍ        ʃajʔiŋ\n",
       "11  7:145:12        فَخُذۡهَا      faxuðha:\n",
       "12  7:145:13        بِقُوَّةٍ     biquwwati\n",
       "13  7:145:14         وَأۡمُرۡ       w̃aʔmur\n",
       "14  7:145:15         قَوۡمَكَ       qawmaka\n",
       "15  7:145:16      يَأۡخُذُواۡ      jaʔxuðu:\n",
       "16  7:145:17  بِأَحۡسَنِهَا‌ۚ  biʔaħsaniha:\n",
       "17  7:145:18    سَأُوۡرِيكُمۡ    saʔuri:kum\n",
       "18  7:145:19            دَارَ         da:ra\n",
       "19  7:145:20   ٱلۡفَـٰسِقِينَ    lfa:siqi:n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.phonemizer import Phonemizer\n",
    "\n",
    "ref = \"7:145\"\n",
    "res = Phonemizer().phonemize(ref, stops=[\"verse\"])\n",
    "res.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phonemized output saved to: out\\phonemized_v5\\1.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\2.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\3.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\4.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\5.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\6.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\7.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\8.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\9.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\10.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\11.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\12.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\13.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\14.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\15.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\16.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\17.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\18.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\19.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\20.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\21.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\22.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\23.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\24.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\25.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\26.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\27.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\28.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\29.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\30.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\31.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\32.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\33.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\34.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\35.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\36.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\37.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\38.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\39.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\40.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\41.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\42.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\43.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\44.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\45.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\46.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\47.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\48.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\49.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\50.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\51.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\52.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\53.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\54.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\55.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\56.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\57.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\58.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\59.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\60.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\61.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\62.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\63.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\64.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\65.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\66.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\67.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\68.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\69.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\70.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\71.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\72.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\73.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\74.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\75.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\76.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\77.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\78.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\79.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\80.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\81.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\82.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\83.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\84.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\85.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\86.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\87.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\88.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\89.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\90.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\91.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\92.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\93.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\94.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\95.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\96.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\97.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\98.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\99.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\100.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\101.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\102.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\103.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\104.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\105.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\106.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\107.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\108.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\109.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\110.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\111.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\112.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\113.txt\n",
      "Phonemized output saved to: out\\phonemized_v5\\114.txt\n"
     ]
    }
   ],
   "source": [
    "from core.helpers import phonemize_and_save\n",
    "\n",
    "for i in range(1,115):\n",
    "    phonemize_and_save(str(i), stops=[\"verse\", \"preferred_stop\", \"compulsory_stop\"], \n",
    "    output_dir=\"out/phonemized_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phonemized output saved to: out\\phonemized_v5\\1-114.txt\n"
     ]
    }
   ],
   "source": [
    "from core.helpers import phonemize_and_save\n",
    "s=\"1-114\"\n",
    "phonemize_and_save(f\"{s}\", \n",
    "    stops=[\"verse\", \"preferred_stop\", \"compulsory_stop\"], \n",
    "    output_dir=\"out/phonemized_v5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.helpers import compare_files\n",
    "compare_files(f\"out/phonemized_v4/1-114.txt\", \n",
    "f\"out/phonemized_v5/1-114.txt\",    \n",
    "ignore_whitespace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
